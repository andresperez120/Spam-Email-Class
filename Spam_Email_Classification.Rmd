---
title: "Spam_Email_Classification"
output: html_document
date: "2025-01-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Installing Required Packages

```{r, message=FALSE}
library(dplyr)
library(tm)
library(SnowballC)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(e1071)
library(ROCR)
```

### Downloading the Data

```{r}
emails = read.csv("/Users/andresperez/Desktop/Personal/PROJECTS/R_Projects/Spam_Email_Classification/emails.csv",stringsAsFactors=FALSE)

```

# Part 1: Exploratory Data Analysis & Exploration

### Number of Emails by Spam/Non-Spam

```{r}
# Group emails by spam status and count
spam_counts = emails %>% group_by(spam) %>% summarize(n=n())

print(spam_counts)


```

There are a total of 5730 emails in this dataset. However, the data must be cleaned so that the `spam` column only contains 0's and 1's and not anything else. This includes removing NA's and non-numeric rows.

```{r}
# Filter the data to include only rows where spam is "0" or "1"
emails <- emails %>% filter(spam %in% c("0", "1"))

# Verify the cleaned data
spam_counts_cleaned <- emails %>% group_by(spam) %>% summarize(n = n())

print(spam_counts_cleaned)

```

Now, there is a total of 5726 emails in our dataset that are labeled as either spam(1) or no spam(0).

### First Email Content

```{r}
# Let's revise the content of the first email
cat("Content of the first email:\n", emails$text[1])
```

### Maximum Number of Characters

```{r}
# Calculate the character count for each email
emails$char.count <- nchar(emails$text)

# Find the maximum character count
max_char_count <- max(emails$char.count)

# Display the maximum character count
cat("Maximum number of characters in an email:", max_char_count, "\n")


```

### Row of Email with Most Characters

```{r}
# Find the row with the maximum character count
max_char_row <- which.max(emails$char.count)

# Display the row number and corresponding character count
cat("Row of the email with the most characters:", max_char_row, "\n")
cat("Character count in this email:", emails$char.count[max_char_row], "\n")

# Display a summary of character counts
cat("Summary of character counts:\n")
print(summary(emails$char.count))

```

### Minimum Number of Characters

```{r}
# Find the minimum character count
min_char_count <- min(emails$char.count)

# Display the minimum character count
cat("Minimum number of characters in an email:", min_char_count, "\n")

```

### Row of Email with Least Characters

```{r}
# Find the row with the minimum character count
min_char_row <- which.min(emails$char.count)

# Display the row number and corresponding character count
cat("Row of the email with the least characters:", min_char_row, "\n")
cat("Character count in this email:", emails$char.count[min_char_row], "\n")

```

### 

## Document - Term Matrix (DTM)

### Preprocess Text Data

```{r}
# Create a text corpus and preprocess the data
corpus <- VCorpus(VectorSource(emails$text))
corpus <- tm_map(corpus, content_transformer(tolower)) # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation)           # Remove punctuation
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stopwords
corpus <- tm_map(corpus, stemDocument)                # Perform stemming
cat("Text preprocessing completed.\n")

```

### Create DTM

```{r}
# Create the document-term matrix (DTM)
dtm <- DocumentTermMatrix(corpus)

# Display the structure of the DTM
cat("Document-Term Matrix (DTM):\n")
print(dtm)


```

### Number of Stepwords

```{r}
# Display the number of stopwords used
cat("Number of stopwords in English:", length(stopwords("english")), "\n")
```

## Sparse DTM

### Reduce Sparse Terms

```{r}
# Create a sparse DTM by removing terms with low frequency
spdtm <- removeSparseTerms(dtm, 0.995)

# Display the structure of the sparse DTM
cat("Sparse Document-Term Matrix (spDTM):\n")
print(spdtm)

```

## Word Frequency Analysis

### Most Frequent Word Stem

```{r}
# Convert the sparse DTM to a data frame
emailsSparse <- as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) <- make.names(colnames(emailsSparse))

# Find the most frequent word stem
stem_freq_max <- names(which.max(colSums(emailsSparse)))

# Display the most frequent word stem
cat("Most frequent word stem:", stem_freq_max, "\n")

```

### Word Stem in Non-Spam Emails ( $\geq$ 5000 Occurrences)

```{r}
# Add the spam column to emailsSparse
emailsSparse$spam <- as.numeric(emails$spam)
emailsSparse <- emailsSparse[!is.na(emailsSparse$spam), ]

# Count word stems appearing at least 5000 times in non-spam emails
nonspam_word_count <- sum(
  colSums(emailsSparse[emailsSparse$spam == 0, sapply(emailsSparse, is.numeric)]) >= 5000
)

# Display the count
cat("Number of word stems appearing at least 5000 times in non-spam emails:", nonspam_word_count, "\n")

```

### Word Stems in Spam Emails ( $\geq$ 1000 Occurrences)

```{r}
# Count word stems appearing at least 1000 times in spam emails
spam_word_count <- sum(
  colSums(emailsSparse[emailsSparse$spam == 1, sapply(emailsSparse, is.numeric)]) >= 1000
)

# Display the count
cat("Number of word stems appearing at least 1000 times in spam emails:", spam_word_count, "\n")

```

## 

# Part 2: Model Building & Evaluation

## Split the Data into Training and Testing Sets

```{r}
# Create a 70/30 split using caret
set.seed(123)
train_index <- createDataPartition(emailsSparse$spam, p = 0.7, list = FALSE)
train_data <- emailsSparse[train_index, ]
test_data <- emailsSparse[-train_index, ]

# Check split sizes
cat("Training Set Size:", nrow(train_data), "\n")
cat("Testing Set Size:", nrow(test_data), "\n")

```

## Logistic Regression

### Train the Model

```{r}
# Train Logistic Regression
spamLog = glm(spam~.,data=train_data, family=binomial)

```

### Evaluate on the Training Set

```{r}
# Predict probabilities on the training set
log_pred_train <- predict(spamLog, type = "response")

# Calculate accuracy with a threshold of 0.5
log_train_accuracy <- mean((log_pred_train >= 0.5) == train_data$spam)

# Print accuracy
cat("Logistic Regression Training Accuracy:", log_train_accuracy, "\n")

```

### Evaluate on the Test Set

```{r}
# Predict probabilities on the test set
log_pred_test <- predict(spamLog, newdata = test_data, type = "response")

# Calculate accuracy with a threshold of 0.5
log_test_accuracy <- mean((log_pred_test >= 0.5) == test_data$spam)

# Print accuracy
cat("Logistic Regression Test Accuracy:", log_test_accuracy, "\n")

```

## CART (Decision Tree)

### Train the Model

```{r}
spam_cart <- rpart(spam ~ ., data = train_data, method = "class")

```

### Visualizing the Decision Tree

```{r}
rpart.plot(spam_cart)

```

### Evaluate on the Train Set

```{r}
# Predict probabilities
cart_pred_train <- predict(spam_cart, type = "prob")[, 2]
# Calculate accuracy
cart_train_accuracy <- mean((cart_pred_train >= 0.5) == train_data$spam)
cat("CART Training Accuracy:", cart_train_accuracy, "\n")

```

### Evaluate on the Test Set

```{r}
# Predict probabilities
cart_pred_test <- predict(spam_cart, newdata = test_data, type = "prob")[, 2]
# Calculate accuracy
cart_test_accuracy <- mean((cart_pred_test >= 0.5) == test_data$spam)
cat("CART Test Accuracy:", cart_test_accuracy, "\n")

```

## Random Forest

### Train the Model

```{r}
train_data$spam <- as.factor(train_data$spam)
test_data$spam <- as.factor(test_data$spam)

set.seed(123)
spam_rf <- randomForest(spam ~ ., data = train_data)
```

### Evaluate on the Train & Test Set

```{r}
# Predict on training set
rf_pred_train <- predict(spam_rf, type = "response")

# Predict on testing set
rf_pred_test <- predict(spam_rf, newdata = test_data, type = "response")

```

### Training Accuracy

```{r}
# Calculate training accuracy
rf_train_accuracy <- mean(rf_pred_train == train_data$spam)
cat("Random Forest Training Accuracy:", rf_train_accuracy, "\n")

```

### Testing Accuracy

```{r}
# Calculate testing accuracy
rf_test_accuracy <- mean(rf_pred_test == test_data$spam)
cat("Random Forest Test Accuracy:", rf_test_accuracy, "\n")

```

### Confusion Matrix

```{r}
# Confusion matrix for testing set
conf_matrix <- confusionMatrix(rf_pred_test, test_data$spam)
print(conf_matrix)

```

## Calculate AUC for Each Model

### Logistic Regression

```{r}
# Get probabilities for the positive class (e.g., "1")
log_prob_test <- predict(spamLog, newdata = test_data, type = "response")

# Ensure Test$spam is numeric
test_data$spam <- as.numeric(as.character(test_data$spam))

# Create prediction object
spamLog.predictionTest <- prediction(as.numeric(log_prob_test), test_data$spam)

# Calculate AUC
spamLog.auc <- as.numeric(performance(spamLog.predictionTest, "auc")@y.values)
cat("Logistic Regression Test AUC:", spamLog.auc, "\n")

# Plot the ROC curve
spamLog.perf <- performance(spamLog.predictionTest, "tpr", "fpr")
plot(spamLog.perf, col = "darkgreen", main = "ROC Curve for Logistic Regression", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")  # Add a diagonal line




```

### CART

```{r}
# Get probabilities for the positive class (e.g., "1")
cart_prob_test <- predict(spam_cart, newdata = test_data, type = "prob")[, 2]

# Create prediction and performance objects
cart_pred_obj <- prediction(cart_prob_test, test_data$spam)
cart_perf <- performance(cart_pred_obj, "tpr", "fpr")

# Plot the ROC curve
plot(cart_perf, col = "blue", main = "ROC Curve for CART", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")  # Add a diagonal line

# Calculate AUC
cart_auc <- as.numeric(performance(cart_pred_obj, "auc")@y.values)
cat("CART Test AUC:", cart_auc, "\n")

```

### Random Forest

```{r}
# Get probabilities for the positive class (e.g., "1")
rf_prob_test <- predict(spam_rf, newdata = test_data, type = "prob")[, 2]

# Create prediction and performance objects
rf_pred_obj <- prediction(rf_prob_test, test_data$spam)
rf_perf <- performance(rf_pred_obj, "tpr", "fpr")

# Plot the ROC curve
plot(rf_perf, col = "blue", main = "ROC Curve for Random Forest", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")  # Add a diagonal line

# Calculate AUC
rf_auc <- as.numeric(performance(rf_pred_obj, "auc")@y.values)
cat("Random Forest Test AUC:", rf_auc, "\n")
```

## Compare Model Performance - Accuracy

```{r}
# Logistic Regression Accuracy
log_accuracy <- mean((log_prob_test >= 0.5) == test_data$spam)

# CART Accuracy
cart_accuracy <- mean((cart_prob_test >= 0.5) == test_data$spam)

# Random Forest Accuracy
rf_accuracy <- mean((rf_prob_test >= 0.5) == test_data$spam)

```

### Compare Model Performance - AUC

```{r}
# Logistic Regression AUC
spamLog.auc  # From previous calculation

# CART AUC
cart_auc  # From previous calculation

# Random Forest AUC
rf_auc  # From previous calculation

```

### Summary Table

```{r}
results <- data.frame(
  Model = c("Logistic Regression", "CART", "Random Forest"),
  Accuracy = c(log_accuracy, cart_accuracy, rf_accuracy),
  AUC = c(spamLog.auc, cart_auc, rf_auc)
)
print(results)

```

## Conclusion

#### In this analysis, three models—Logistic Regression, CART, and Random Forest—were trained and evaluated to classify emails as spam or non-spam. The performance of each model was assessed using two key metrics: **Accuracy** and **AUC (Area Under the ROC Curve)**.

------------------------------------------------------------------------

#### **Model Performance Overview**

1.  **Logistic Regression**:

    -   Accuracy: **81.25%**

    -   AUC: **0.848**

    -   As a simple linear model, Logistic Regression performed reasonably well. Its ability to provide interpretable results makes it a viable choice for tasks where simplicity and interpretability are prioritized. However, its lower accuracy and AUC indicate it struggles with capturing complex relationships in the dataset.

2.  **CART (Classification and Regression Trees)**:

    -   Accuracy: **94.06%**

    -   AUC: **0.967**

    -   CART significantly outperformed Logistic Regression in both accuracy and AUC. Its interpretability—through decision trees—is a major strength, allowing easy understanding of the classification rules. However, decision trees are prone to overfitting, which can reduce generalization ability on unseen data.

3.  **Random Forest**:

    -   Accuracy: **98.78%**

    -   AUC: **0.999**

    -   Random Forest emerged as the best-performing model, achieving the highest accuracy and AUC. Its ensemble approach effectively handles complex relationships and reduces overfitting compared to a single decision tree. This makes it a robust choice for classification tasks requiring high predictive power.

------------------------------------------------------------------------

#### **Comparison and Final Recommendation**

The **Random Forest** model demonstrated exceptional performance with an accuracy of **98.78%** and an AUC of **0.999**, surpassing both Logistic Regression and CART in all evaluated metrics. CART also performed well, particularly in terms of interpretability, while Logistic Regression offered a simpler alternative with moderate performance.

Given the results, **Random Forest is recommended** for spam email classification due to its superior predictive power and ability to handle complex patterns in the dataset. While it is computationally more intensive, its accuracy and AUC justify its use for this task. For scenarios where interpretability is key, CART may serve as a strong secondary option. Logistic Regression, while the simplest, is not ideal for this dataset due to its comparatively lower performance.
